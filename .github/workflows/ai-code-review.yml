name: Universal AI Code Review

on:
  push:
    branches: [ main, master, development ]
  pull_request:
    branches: [ main, master, development ]

jobs:
  ai-code-review:
    runs-on: ubuntu-latest
    
    env:
      OLLAMA_HOST: http://localhost:11434
    
    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install Dependencies
      run: |
        # Install Ollama
        curl -fsSL https://ollama.com/install.sh | sh
        ollama serve &
        
        # Wait for Ollama service
        for i in {1..30}; do
          if curl -s http://localhost:11434/api/health >/dev/null; then
            echo "âœ“ Ollama is running"
            break
          fi
          echo "Waiting for Ollama to start..."
          sleep 2
        done
        
        # Install Python dependencies
        python -m pip install --upgrade pip
        pip install requests flask flask-cors radon lizard pylint
        
        # Pull Llama model
        ollama pull llama2:latest
        ollama list | grep llama2

    - name: Create Review System
      run: |
        cat > code_review_system.py << 'EOF'
import os
import sys
import json
import traceback
from dataclasses import dataclass, asdict
from typing import Dict, List, Optional, Any
import requests
from flask import Flask, request, jsonify
from threading import Thread
import radon.raw as raw
from radon.visitors import ComplexityVisitor
import lizard

app = Flask(__name__)

@dataclass
class CodeMetrics:
    loc: int = 0
    lloc: int = 0
    sloc: int = 0
    comments: int = 0
    function_count: int = 0
    avg_function_length: float = 0
    complexity: Dict[str, int] = None
    functions: List[Dict] = None
    
    def __post_init__(self):
        if self.complexity is None:
            self.complexity = {}
        if self.functions is None:
            self.functions = []

class CodeReviewer:
    def __init__(self, ollama_host: str):
        self.ollama_host = ollama_host
        
    def analyze_metrics(self, code: str) -> CodeMetrics:
        """Generate comprehensive code metrics."""
        metrics = CodeMetrics()
        
        try:
            # Basic metrics
            raw_metrics = raw.analyze(code)
            metrics.loc = raw_metrics.loc
            metrics.lloc = raw_metrics.lloc
            metrics.sloc = raw_metrics.sloc
            metrics.comments = raw_metrics.comments
            
            # Complexity metrics
            complexity = ComplexityVisitor.from_code(code)
            metrics.complexity = {block.name: block.complexity for block in complexity.functions}
            
            # Function metrics
            analysis = lizard.analyze_file.analyze_source_code("temp.py", code)
            metrics.function_count = len(analysis.function_list)
            metrics.avg_function_length = (
                sum(f.length for f in analysis.function_list) / len(analysis.function_list)
                if analysis.function_list else 0
            )
            metrics.functions = [{
                'name': f.name,
                'length': f.length,
                'complexity': f.cyclomatic_complexity,
                'tokens': f.token_count,
                'parameters': f.parameter_count
            } for f in analysis.function_list]
            
        except Exception as e:
            print(f"Error analyzing metrics: {str(e)}")
            
        return metrics
    
    def generate_diagrams(self, metrics: CodeMetrics) -> Dict[str, str]:
        """Generate Mermaid diagrams from metrics."""
        diagrams = {}
        
        # Code composition
        diagrams['composition'] = f"""
        pie title Code Composition
            "Logical Lines" : {metrics.lloc}
            "Comments" : {metrics.comments}
            "Empty Lines" : {metrics.loc - metrics.sloc}
        """
        
        # Function complexity
        if metrics.functions:
            diagrams['complexity'] = """
            xychart-beta
                title "Function Complexity"
                x-axis [{}]
                y-axis "Complexity" 0 --> 20
                bar [{}]
            """.format(
                ", ".join(f'"{f["name"]}"' for f in metrics.functions),
                ", ".join(str(f["complexity"]) for f in metrics.functions)
            )
        
        # Function metrics
        if metrics.function_count > 0:
            diagrams['functions'] = f"""
            stateDiagram-v2
                [*] --> Overview
                Overview : {metrics.function_count} Functions
                Overview : Avg Length: {metrics.avg_function_length:.1f} lines
                Overview --> Details
                Details : Parameters: {sum(f['parameters'] for f in metrics.functions)}
                Details : Tokens: {sum(f['tokens'] for f in metrics.functions)}
            """
        
        return diagrams
    
    def generate_review_prompt(self, code: str, filename: str) -> str:
        """Generate AI review prompt."""
        return f"""As a senior software engineer, perform a comprehensive code review analyzing:

1. Code Quality
   - Design patterns and architecture
   - Code organization and maintainability
   - Naming conventions and readability
   - Documentation completeness

2. Performance
   - Algorithmic efficiency
   - Resource usage
   - Potential bottlenecks
   - Optimization opportunities

3. Security
   - Input validation
   - Authentication/authorization
   - Data protection
   - Security best practices

4. Reliability
   - Error handling
   - Edge cases
   - Resource cleanup
   - Testing coverage

Structure your review with:
- Summary
- Critical Issues (High/Medium/Low severity)
- Specific Recommendations
- Best Practices

Code to review:
```
{code}
```
"""
    
    def get_ai_review(self, code: str, filename: str) -> Optional[str]:
        """Get AI review from Ollama."""
        try:
            response = requests.post(
                f"{self.ollama_host}/api/generate",
                json={
                    "model": "llama2",
                    "prompt": self.generate_review_prompt(code, filename),
                    "stream": False
                },
                timeout=300
            )
            
            if response.status_code == 200:
                return response.json()['response']
            
            print(f"Ollama error: {response.status_code} - {response.text}")
            return None
            
        except Exception as e:
            print(f"Error getting AI review: {str(e)}")
            return None

@app.route('/api/review', methods=['POST'])
def review_code():
    """API endpoint for code review."""
    try:
        data = request.json
        code = data['code']
        filename = data.get('fileName', 'unknown.py')
        
        reviewer = CodeReviewer(os.getenv('OLLAMA_HOST', 'http://localhost:11434'))
        
        # Generate metrics and diagrams
        metrics = reviewer.analyze_metrics(code)
        diagrams = reviewer.generate_diagrams(metrics)
        
        # Get AI review
        review_text = reviewer.get_ai_review(code, filename)
        
        return jsonify({
            'fileName': filename,
            'reviewResults': {
                'metrics': asdict(metrics),
                'diagrams': diagrams,
                'comprehensive_review': review_text
            }
        })
        
    except Exception as e:
        return jsonify({
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

def run_server():
    """Run Flask server."""
    app.run(host='0.0.0.0', port=5000)

if __name__ == '__main__':
    server_thread = Thread(target=run_server, daemon=True)
    server_thread.start()
    print("Review server started on port 5000")
EOF

    - name: Start Review System
      run: |
        python code_review_system.py &
        echo "Waiting for server to start..."
        sleep 5
        
        # Verify server
        if curl -s http://localhost:5000/api/review -X POST \
           -H "Content-Type: application/json" \
           -d '{"code":"def test(): pass","fileName":"test.py"}' > /dev/null; then
          echo "âœ“ Review server operational"
        else
          echo "Ã— Failed to verify review server"
          exit 1
        fi
    
    - name: Run Reviews
      run: |
        python3 << 'EOF'
import os
import json
import requests
from pathlib import Path
from typing import List, Dict, Any, Optional

def is_source_file(path: Path) -> bool:
    """Check if file is a source code file."""
    source_extensions = {
        '.py', '.js', '.jsx', '.ts', '.tsx', '.java',
        '.cpp', '.c', '.rb', '.go', '.php', '.swift',
        '.kt', '.rs', '.html', '.css', '.scss'
    }
    return path.suffix.lower() in source_extensions

def should_skip(path: Path) -> bool:
    """Check if path should be skipped."""
    skip_patterns = {
        '.git', 'node_modules', 'dist', 'build',
        'venv', '.env', 'coverage', '__pycache__',
        'test', 'tests', 'tmp', 'temp'
    }
    return any(part in skip_patterns for part in path.parts)

def review_file(path: Path) -> Optional[Dict[str, Any]]:
    """Send file for review."""
    try:
        code = path.read_text(encoding='utf-8')
        response = requests.post(
            'http://localhost:5000/api/review',
            json={'code': code, 'fileName': path.name},
            timeout=300
        )
        return response.json() if response.status_code == 200 else None
    except Exception as e:
        print(f"Error reviewing {path}: {e}")
        return None

def generate_report(reviews: List[Dict[str, Any]]) -> str:
    """Generate markdown report from reviews."""
    if not reviews:
        return "# No files were reviewed."
    
    # Calculate repository metrics
    total_loc = sum(r['reviewResults']['metrics']['loc'] for r in reviews if r)
    total_funcs = sum(r['reviewResults']['metrics']['function_count'] for r in reviews if r)
    total_complexity = sum(
        sum(c for c in r['reviewResults']['metrics']['complexity'].values())
        for r in reviews if r
    )
    
    report = [
        "# ðŸ¤– AI Code Review Report\n",
        f"## Repository Overview\n",
        f"- Files Analyzed: {len(reviews)}",
        f"- Total Lines: {total_loc:,}",
        f"- Total Functions: {total_funcs}",
        f"- Average Complexity: {total_complexity/total_funcs:.2f if total_funcs else 0}\n",
        "```mermaid",
        "pie title Repository Composition",
        f'    "Code" : {total_loc}',
        f'    "Functions" : {total_funcs}',
        f'    "Complexity" : {int(total_complexity)}',
        "```\n"
    ]
    
    # Add individual file reviews
    for review in reviews:
        if not review:
            continue
            
        file_name = review['fileName']
        results = review['reviewResults']
        
        report.extend([
            f"## File: `{file_name}`\n",
            "### Metrics\n",
            f"- Lines of Code: {results['metrics']['loc']}",
            f"- Functions: {results['metrics']['function_count']}",
            f"- Comments: {results['metrics']['comments']}\n"
        ])
        
        # Add diagrams
        for name, diagram in results['diagrams'].items():
            report.extend([
                f"### {name.title()}\n",
                "```mermaid",
                diagram.strip(),
                "```\n"
            ])
        
        # Add AI review
        if results.get('comprehensive_review'):
            report.extend([
                "### AI Review\n",
                results['comprehensive_review'],
                "\n---\n"
            ])
    
    return '\n'.join(report)

def main():
    """Main review process."""
    # Create output directory
    output_dir = Path('code-reviews')
    output_dir.mkdir(exist_ok=True)
    
    # Review files
    reviews = []
    for path in Path('.').rglob('*'):
        if path.is_file() and is_source_file(path) and not should_skip(path):
            print(f"Reviewing: {path}")
            if review := review_file(path):
                reviews.append(review)
    
    # Generate and save report
    report = generate_report(reviews)
    report_path = output_dir / 'review_report.md'
    report_path.write_text(report, encoding='utf-8')
    print(f"Report generated: {report_path}")

if __name__ == '__main__':
    main()
EOF

    - name: Commit Results
      run: |
        git config user.name github-actions
        git config user.email github-actions@github.com
        git add code-reviews/review_report.md || true
        git commit -m "Add AI Code Review Report [skip ci]" || true
        git push || true
